<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://francesco-fortunato.github.io/it/feed.xml" rel="self" type="application/atom+xml"/><link href="https://francesco-fortunato.github.io/it/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-10-12T16:56:24+00:00</updated><id>https://francesco-fortunato.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">How to Build a DQN to Play Atari Breakout (and Actually Get Good Results!)</title><link href="https://francesco-fortunato.github.io/it/blog/2024/dqn-breakout/" rel="alternate" type="text/html" title="How to Build a DQN to Play Atari Breakout (and Actually Get Good Results!)"/><published>2024-10-01T15:00:00+00:00</published><updated>2024-10-01T15:00:00+00:00</updated><id>https://francesco-fortunato.github.io/blog/2024/dqn-breakout</id><content type="html" xml:base="https://francesco-fortunato.github.io/blog/2024/dqn-breakout/"><![CDATA[<p>If you’ve ever wondered how an AI can learn to play games like Atari Breakout, you’re in the right place! In this post, we’ll break down the steps I took to build a Deep Q-Network (DQN) that can play Breakout and smash those blocks with skill.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 d-flex align-items-center justify-content-center" style="height: 100%;"> <figure> <video src="/assets/video/ATARI_Breakout_Eval_model_21700_reward_357-speed.mp4" class="img-fluid rounded z-depth-1" width="200" height="auto" autoplay="" controls=""/> </figure> </div> </div> <p>For a deep dive into the code, feel free to check out my <a href="https://github.com/francesco-fortunato/DQN-breakout">GitHub repository</a>. Let’s get started!</p> <hr/> <h2 id="the-environment-atari-breakout">The Environment: Atari Breakout</h2> <p>Atari Breakout is a timeless classic where you control a paddle to bounce a ball and break bricks. The objective is straightforward: break all the bricks without letting the ball slip past you. To make our lives easier, we’ll leverage OpenAI Gym, which offers a variety of built-in environments for simulating games. For our project, we’ll be using the BreakoutNoFrameskip-v4 environment.</p> <h2 id="the-dqn-model-teaching-the-agent-to-play">The DQN Model: Teaching the Agent to Play</h2> <p>Our DQN model is essentially a neural network that learns to predict the best action to take based on the current game state. In simpler terms, it tries to figure out whether it should move the paddle left, right, or just stay put.</p> <h3 id="cnn-architecture-why-convolutions-are-perfect-for-games">CNN Architecture: Why Convolutions Are Perfect for Games</h3> <p>For our agent to “see” the game properly, we use a Convolutional Neural Network (CNN). CNNs are amazing for image-based tasks because they can automatically pick out important patterns, like where the ball and bricks are.</p> <p>Here’s the architecture we used:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Network-480.webp 480w,/assets/img/Network-800.webp 800w,/assets/img/Network-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/Network.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> CNN Architecture </div> <p>That translates in the following code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">create_q_model</span><span class="p">():</span>
    <span class="c1"># Network defined by the Deepmind paper
</span>    <span class="n">inputs</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">84</span><span class="p">,</span> <span class="mi">4</span><span class="p">,))</span>

    <span class="c1"># Define the first convolutional layer
</span>    <span class="c1"># - 32 filters, each 8x8 in size
</span>    <span class="c1"># - Stride of 4, meaning the filter moves 4 pixels at a time
</span>    <span class="c1"># - ReLU activation function is applied to the output
</span>    <span class="n">layer1</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">"</span><span class="s">relu</span><span class="sh">"</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>

    <span class="c1"># Define the second convolutional layer
</span>    <span class="c1"># - 64 filters, each 4x4 in size
</span>    <span class="c1"># - Stride of 2
</span>    <span class="c1"># - ReLU activation function
</span>    <span class="n">layer2</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">"</span><span class="s">relu</span><span class="sh">"</span><span class="p">)(</span><span class="n">layer1</span><span class="p">)</span>

    <span class="c1"># Define the third convolutional layer
</span>    <span class="c1"># - 64 filters, each 3x3 in size
</span>    <span class="c1"># - Stride of 1
</span>    <span class="c1"># - ReLU activation function
</span>    <span class="n">layer3</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">"</span><span class="s">relu</span><span class="sh">"</span><span class="p">)(</span><span class="n">layer2</span><span class="p">)</span>

    <span class="c1"># Flatten the output from the convolutional layers
</span>    <span class="n">layer4</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Flatten</span><span class="p">()(</span><span class="n">layer3</span><span class="p">)</span>

    <span class="c1"># Define a fully connected layer with 512 neurons
</span>    <span class="c1"># - ReLU activation function
</span>    <span class="n">layer5</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">"</span><span class="s">relu</span><span class="sh">"</span><span class="p">)(</span><span class="n">layer4</span><span class="p">)</span>

    <span class="c1"># Output layer with num_actions neurons (4 in this case for the Breakout game)
</span>    <span class="c1"># - Linear activation function
</span>    <span class="n">action</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">num_actions</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">"</span><span class="s">linear</span><span class="sh">"</span><span class="p">)(</span><span class="n">layer5</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">keras</span><span class="p">.</span><span class="nc">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">action</span><span class="p">)</span>
</code></pre></div></div> <p>Now, you may think: if I have just three moves inside Atari (left, right, still), why are there 4 <code class="language-plaintext highlighter-rouge">num_actions</code>? Well, the fourth action is needed for starting the game. Once the game has started, we don’t want to use that button anymore. For this reason, we use <strong>Atari wrappers</strong> to handle such nuances in the environment.</p> <hr/> <h2 id="atari-breakout-environment-wrappers">Atari Breakout Environment Wrappers</h2> <p>To help the agent interact with the environment effectively, we use several <strong>Atari wrappers</strong>. These wrappers preprocess the frames, manage game resets, and handle special cases like losing a life or needing to press “Fire” to start the game. Some of the nost important wrappers we use to ensure the game environment includes EpisodicLifeEnv to treat life loss as episode end (we have three lifes), MaxAndSkipEnv to skip unnecessary frames, and FrameStack to maintain temporal continuity by stacking multiple frames.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Use the Baseline Atari environment because of Deepmind helper functions
</span><span class="n">env</span> <span class="o">=</span> <span class="nf">make_atari_breakout</span><span class="p">(</span><span class="sh">"</span><span class="s">BreakoutNoFrameskip-v4</span><span class="sh">"</span><span class="p">)</span>
<span class="c1"># Warp the frames, grey scale, stake four frame and scale to smaller ratio
</span><span class="n">env</span> <span class="o">=</span> <span class="nf">wrap</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">frame_stack</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">env</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
</code></pre></div></div> <hr/> <h3 id="pre-processing-of-the-images">Pre-processing of the Images</h3> <p>To ensure our neural network can learn effectively from the game frames, we need to preprocess the images observed in the environment before they are fed into the network.</p> <ul> <li><strong>Frame Stacking</strong>: The wrapper stacks multiple frames together (in this case, four frames) to provide the neural network with a sense of motion and temporal continuity. This helps the agent make decisions based on the recent history of observations.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 d-flex align-items-center justify-content-center" style="height: 100%;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/frame_00_delay-0.02s-480.webp 480w,/assets/img/frame_00_delay-0.02s-800.webp 800w,/assets/img/frame_00_delay-0.02s-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/frame_00_delay-0.02s.jpg" class="img-fluid rounded z-depth-1" width="200" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> With just one frame, it's impossible to tell where the ball is going. </div> <p>Below, you can see how stacking four frames together provides a clearer understanding of the ball’s movement:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <div style="display: flex; justify-content: space-around;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/frame_00_delay-0.02s-480.webp 480w,/assets/img/frame_00_delay-0.02s-800.webp 800w,/assets/img/frame_00_delay-0.02s-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/frame_00_delay-0.02s.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/frame_01_delay-0.02s-480.webp 480w,/assets/img/frame_01_delay-0.02s-800.webp 800w,/assets/img/frame_01_delay-0.02s-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/frame_01_delay-0.02s.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/frame_02_delay-0.02s-480.webp 480w,/assets/img/frame_02_delay-0.02s-800.webp 800w,/assets/img/frame_02_delay-0.02s-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/frame_02_delay-0.02s.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/frame_03_delay-0.02s-480.webp 480w,/assets/img/frame_03_delay-0.02s-800.webp 800w,/assets/img/frame_03_delay-0.02s-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/frame_03_delay-0.02s.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <div class="caption"> With a stack of four frames, we can easily understand where the ball is going. </div> <ul> <li> <p><strong>Grayscale Conversion</strong>: Grayscale conversion helps reduce the computational complexity of the neural network, making it faster to train.</p> </li> <li> <p><strong>Scaling to a Smaller Ratio</strong>: It crops the images to remove unnecessary parts, like the score and other additional information that are not useful for the computation.</p> </li> </ul> <p>Below is the pre-processing flow showcasing the steps:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <div style="display: flex; justify-content: space-around; align-items: center; height: 100%;"> <div style="width: 20%; text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/frame_00_delay-0.02s-480.webp 480w,/assets/img/frame_00_delay-0.02s-800.webp 800w,/assets/img/frame_00_delay-0.02s-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/frame_00_delay-0.02s.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Original Frame</div> </div> <div style="width: 20%; text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cropped-480.webp 480w,/assets/img/cropped-800.webp 800w,/assets/img/cropped-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/cropped.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Cropped Frame</div> </div> <div style="width: 20%; text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/grey-480.webp 480w,/assets/img/grey-800.webp 800w,/assets/img/grey-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/grey.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Grayscale Frame</div> </div> </div> </div> </div> <div class="caption"> Pre-processing flow: from the original frame to cropped and grayscale versions. </div> <hr/> <h2 id="training-process">Training Process</h2> <h3 id="exploration-exploitation-trade-off">Exploration-Exploitation Trade-off</h3> <p>In reinforcement learning, agents face the dilemma of whether to explore new actions or exploit known ones to maximize rewards. This <strong>exploration-exploitation trade-off</strong> is crucial for balancing the discovery of potentially better actions and the exploitation of known optimal actions.</p> <p>We define <strong>\(\epsilon\)</strong> as a function of the number of frames the agent has seen. For the first 50,000 frames, the agent only explores, setting <strong>\(\epsilon = 1\)</strong>. Over the next 1 million frames, <strong>\(\epsilon\)</strong> is linearly decreased to <strong>0.1</strong>, meaning the agent gradually starts to exploit its knowledge more. While DeepMind maintains <strong>\(\epsilon = 0.1\)</strong>, I chose to reduce it to <strong>\(\epsilon = 0.01\)</strong> over the remaining 24 million frames, as suggested by the <a href="https://openai.com/research/openai-baselines-dqn">OpenAI Baselines for DQN</a> (note that in the accompanying plot, the maximum number of frames is shown as 2 million for demonstration purposes).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/epsilon-480.webp 480w,/assets/img/epsilon-800.webp 800w,/assets/img/epsilon-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/epsilon.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h3 id="configuration-parameters">Configuration Parameters</h3> <p>To kick things off, let’s outline the key configuration parameters that will set the foundation for our training process. These parameters are essential for controlling various aspects of the learning algorithm:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Configuration parameters for the whole setup
</span><span class="n">seed</span> <span class="o">=</span> <span class="mi">42</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.99</span>  <span class="c1"># Discount factor for past rewards
</span><span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1.0</span>  <span class="c1"># Epsilon greedy parameter
</span><span class="n">epsilon_min</span> <span class="o">=</span> <span class="mf">0.1</span>  <span class="c1"># Minimum epsilon greedy parameter
</span><span class="n">epsilon_final</span> <span class="o">=</span> <span class="mf">0.01</span>  <span class="c1"># Minimum epsilon greedy parameter
</span><span class="n">epsilon_max</span> <span class="o">=</span> <span class="mf">1.0</span>  <span class="c1"># Maximum epsilon greedy parameter
</span><span class="n">epsilon_interval</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">epsilon_max</span> <span class="o">-</span> <span class="n">epsilon_min</span>
<span class="p">)</span>  <span class="c1"># Rate at which to reduce chance of random action being taken
</span><span class="n">epsilon_interval_2</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">epsilon_min</span> <span class="o">-</span> <span class="n">epsilon_final</span>
<span class="p">)</span>  <span class="c1"># Rate at which to reduce chance of random action being taken after 1 million frames
</span>
<span class="c1"># Number of frames to take random action and observe output
</span><span class="n">epsilon_random_frames</span> <span class="o">=</span> <span class="mf">50000.0</span>   <span class="c1"># Number of frames with epsilon set to 1.0
# Number of frames for exploration
</span><span class="n">epsilon_greedy_frames</span> <span class="o">=</span> <span class="mf">1000000.0</span> <span class="c1"># Number of frames to linearly decay epsilon from 1 to 0.1
</span><span class="n">epsilon_final_frames</span> <span class="o">=</span> <span class="mf">24000000.0</span> <span class="c1"># Number of frames to linearly decay epsilon from 0.1 to 0.01
</span></code></pre></div></div> <h3 id="pseudocode">Pseudocode</h3> <p>Now, let’s dive into the pseudocode for our Deep Q-learning algorithm with experience replay. This gives us a clear roadmap of the algorithm’s structure and flow:</p> <pre><code class="language-pseudocode">\begin{algorithm}
\caption{Deep Q-learning with Experience Replay}
\begin{algorithmic}
\STATE Initialize replay memory $$D$$ to capacity $$N$$
\STATE Initialize action-value function $$Q$$ with random weights $$θ$$
\STATE Initialize target action-value function $$Q̂$$ with weights $$θ⁻ = θ$$
\FOR{episode $$= 1$$ \TO $$M$$}
    \STATE Initialize sequence $$s₁ = \{x₁\}$$ and preprocessed sequence $$ϕ₁ = ϕ(s₁)$$
    \FOR{$$t = 1$$ \TO $$T$$}
        \STATE With probability $$ε$$ select a random action $$aₜ$$
        \STATE otherwise select $$aₜ = $$ argmaxₐ$$ Q(ϕ(sₜ), a; θ)$$
        \STATE Execute action $$aₜ$$ in emulator and observe reward $$rₜ$$ and image $$xₜ₊₁$$
        \STATE Set $$sₜ₊₁ = sₜ, aₜ, xₜ₊₁$$ and preprocess $$ϕₜ₊₁ = ϕ(sₜ₊₁)$$
        \STATE Store transition $$(ϕₜ, aₜ, rₜ, ϕₜ₊₁)$$ in $$D$$
        \STATE Sample random minibatch of transitions $$(ϕₕ, aₕ, rₕ, ϕₕ₊₁)$$ from $$D$$
        \STATE Set $$yₕ = rₕ if episode terminates at step h + 1; otherwise rₕ + γ maxₐ Q̂(ϕₕ₊₁, a'; θ⁻)$$
        \STATE Perform a gradient descent step on $$(yₕ - Q(ϕₕ, aₕ; θ))²$$ with respect to the network parameters $$θ$$
        \STATE Every $$C$$ steps, set $$Q̂ = Q$$
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}
</code></pre> <p>This pseudocode outlines how we initialize our environment, select actions based on the epsilon-greedy strategy, and manage experiences in the replay buffer. Let’s explain how does it works and which are the components we need.</p> <hr/> <h3 id="replay-memory-and-other-parameters">Replay Memory and Other Parameters</h3> <p>One crucial aspect of our training setup is the replay memory. Experiences are stored in a replay buffer, allowing the model to be trained periodically using sampled batches from this buffer. This approach not only helps in maintaining a diverse set of experiences but also stabilizes the learning process.</p> <p>In the spirit of the DeepMind paper, I’ve defined a maximum memory size for the buffer, albeit a smaller one due to computational constraints. Using a replay buffer is beneficial because experiences are often highly correlated temporally. Training directly on consecutive experiences can lead to instability and slow convergence. Without this buffer, an agent risks overfitting to recent experiences, which hampers its ability to generalize effectively.</p> <p>By implementing these configurations and leveraging replay memory, we set our agent up for success in mastering the task at hand.</p> <h4 id="experience-replay-buffers">Experience Replay Buffers</h4> <p>One of the foundational techniques in our reinforcement learning setup for implementing the replay memory is the use of experience replay buffers. These buffers store the agent’s experiences, allowing for more effective training by breaking the correlation between consecutive experiences. Below are the key components of our experience replay setup:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Experience replay buffers
</span><span class="n">action_history</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">state_history</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">state_next_history</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">rewards_history</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">done_history</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">episode_reward_history</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>  <span class="c1"># Size of batch taken from replay buffer
</span><span class="n">max_steps_per_episode</span> <span class="o">=</span> <span class="mi">10000</span>

<span class="c1"># Maximum replay length
# Note: The Deepmind paper suggests 1,000,000; however, this causes memory issues
</span><span class="n">max_memory_length</span> <span class="o">=</span> <span class="mi">100000</span>

<span class="c1"># Train the model after 4 actions
</span><span class="n">update_after_actions</span> <span class="o">=</span> <span class="mi">4</span>

<span class="c1"># How often to update the target network
</span><span class="n">update_target_network</span> <span class="o">=</span> <span class="mi">10000</span>

<span class="n">running_reward</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">episode_count</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">frame_count</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">terminal_life_lost</span> <span class="o">=</span> <span class="bp">False</span>
</code></pre></div></div> <hr/> <h3 id="huber-loss">Huber Loss</h3> <p>One other interesting thing to notice: DeepMind uses the quadratic cost function with error clipping (see page 7 of <a href="https://www.nature.com/articles/nature14236/">Mnih et al. 2015</a>).</p> <blockquote> <p>We also found it helpful to clip the error term from the update […] to be between -1 and 1. Because the absolute value loss function <code class="language-plaintext highlighter-rouge">$$|x|$$</code> has a derivative of -1 for all negative values of <code class="language-plaintext highlighter-rouge">$$x$$</code> and a derivative of 1 for all positive values of <code class="language-plaintext highlighter-rouge">$$x$$</code>, clipping the squared error to be between -1 and 1 corresponds to using an absolute value loss function for errors outside of the (-1,1) interval. This form of error clipping further improved the stability of the algorithm.</p> </blockquote> <p>Why does this improve the stability of the algorithm?</p> <blockquote> <p>In deep networks or recurrent neural networks, error gradients can accumulate during an update and result in very large gradients. These can lead to large updates to the network weights, resulting in an unstable network. In extreme cases, the values of weights can become so large as to overflow and result in NaN values. <a href="https://machinelearningmastery.com/exploding-gradients-in-neural-networks/">Source</a></p> </blockquote> <p>This so-called <strong>exploding gradient problem</strong> can be avoided to some extent by clipping the gradients to a certain threshold value if they exceed it: <em>If the true gradient is larger than a critical value (x), just assume it is (x).</em> The derivative of the green curve does not increase (or decrease) for (x &gt; 1) (or (x &lt; -1)).</p> <p>Error clipping can be easily implemented in TensorFlow by using the Huber loss function <code class="language-plaintext highlighter-rouge">tf.losses.huber_loss</code>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/huber-480.webp 480w,/assets/img/huber-800.webp 800w,/assets/img/huber-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/huber.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="implementing-huber-loss">Implementing Huber Loss</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Using huber loss for stability
</span><span class="n">loss_function</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">losses</span><span class="p">.</span><span class="nc">Huber</span><span class="p">()</span>

<span class="c1"># In the DeepMind paper, they use RMSProp; however, the Adam optimizer improves training time
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.00025</span><span class="p">,</span> <span class="n">clipnorm</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
</code></pre></div></div> <hr/> <h3 id="q-values-and-the-bellman-equation">Q-Values and the Bellman Equation</h3> <p>One of the foundational concepts in reinforcement learning field that shapes how intelligent agents make decisions is the <strong>Bellman equation</strong>. This powerful equation allows agents to evaluate the quality of actions they can take in various states, essentially providing a roadmap for optimal decision-making.</p> <p>The Bellman equation is expressed as follows:</p> \[Q(s, a) = r + \gamma \cdot \max_{a'} Q(s', a')\] <p>Let’s break down its components to understand what each term represents:</p> <ul> <li><strong>(Q(s, a))</strong>: This is the Q-value for a specific action (a) in a given state (s). It serves as a crucial indicator of how good an action is in a particular situation, guiding the agent toward better choices.</li> <li> <p><strong>(r)</strong>: This term signifies the immediate reward the agent receives after taking action (a) in state (s). It provides instant feedback from the environment, telling the agent how well it performed.</p> </li> <li> <p><strong>(\gamma)</strong>: Known as the discount factor, this parameter ranges between 0 and 1. It helps determine how much importance the agent should give to future rewards compared to immediate ones, fostering a balance between short-term gains and long-term strategies.</p> </li> <li><strong>(\max_{a’} Q(s’, a’))</strong>: This part captures the maximum Q-value achievable in the next state (s’). It reflects the agent’s estimate of the best possible future reward by considering all available actions.</li> </ul> <p>The Bellman equation is a guiding principle for reinforcement learning algorithms, blending theoretical concepts with practical implementations.</p> <h4 id="additional-consideration-in-implementation">Additional Consideration in Implementation</h4> <p>In our implementation, we make a slight modification to the Bellman equation to better accommodate the dynamics of the environment:</p> \[Q(s, a) = r + \gamma \cdot \max_{a'} Q(s', a') \cdot (1 - \text{done})\] <p>Here, the addition of the binary flag (\text{done}) indicates whether the episode has terminated after taking the current action. The term ((1 - \text{done})) acts as a filter, allowing the update of Q-values only when the episode is ongoing. If the episode has ended ((\text{done} = 1)), future expected rewards are excluded from the update, as there’s no subsequent state ((s’)) to consider.</p> <p>This adjustment is particularly important in scenarios where the termination of an episode doesn’t signify the end of the learning process. In games like Atari Breakout, losing a life does not mean the game is over; the agent continues to play. The binary flag (\text{done}) ensures that Q-values are updated appropriately based on the episodic dynamics, allowing the agent to learn from its experiences even after setbacks.</p> <p>Let’s illustrate this in simpler terms:</p> \[\text{Set } y_j = \begin{cases} r_j &amp; \text{if episode terminates at step } j+1 \\ r_j + \gamma \max_{a'} Q(s_{j+1}, a'; \theta^-) &amp; \text{otherwise} \end{cases}\] <h4 id="code-representation">Code Representation</h4> <p>In the Python implementation, this concept is reflected in the following line of code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">updated_q_values</span> <span class="o">=</span> <span class="n">rewards_sample</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">done_sample</span><span class="p">)</span> <span class="o">*</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_max</span><span class="p">(</span><span class="n">future_rewards</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <p>This line corresponds to our modified Bellman equation, where we effectively update the Q-values during the agent’s learning process. By incorporating the discount factor and the done flag, we ensure that our Q-learning approach is both effective and adaptable to the nuances of the environment.</p> <h3 id="training-loop">Training Loop</h3> <p>In the heart of our Deep Q-Learning algorithm lies the training loop, where the agent learns from its interactions with the environment. To set this up effectively, we need to create two models: the <strong>primary model</strong> that will be trained and a <strong>target model</strong> for predicting future rewards. Here’s how we set them up:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># The first model makes the predictions for Q-values which are used to
# make a action.
</span><span class="n">model</span> <span class="o">=</span> <span class="nf">create_q_model</span><span class="p">()</span>
<span class="c1"># Build a target model for the prediction of future rewards.
# The weights of a target model get updated every 10000 steps thus when the
# loss between the Q-values is calculated the target Q-value is stable.
</span><span class="n">model_target</span> <span class="o">=</span> <span class="nf">create_q_model</span><span class="p">()</span>
</code></pre></div></div> <p>The target model plays a critical role in mitigating the oscillations and divergence that can occur during training. Instead of using the current model to predict future Q-values, we update the target model’s weights to match those of the training model every 10,000 time steps. This stability is essential for effective learning.</p> <p>To track the training process and check the results, I decided to implement a script that creates or updates a CSV file containing essential statistics:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">csv_filename</span> <span class="o">=</span> <span class="sh">"</span><span class="s">training_stats.csv</span><span class="sh">"</span>
<span class="c1"># Check if the CSV file exists
</span><span class="k">if</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">exists</span><span class="p">(</span><span class="n">csv_filename</span><span class="p">):</span>
    <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">csv_filename</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="sh">'</span><span class="s">r</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="nb">file</span><span class="p">:</span>
        <span class="c1"># CSV file already exists, read the header
</span>        <span class="n">reader</span> <span class="o">=</span> <span class="n">csv</span><span class="p">.</span><span class="nf">reader</span><span class="p">(</span><span class="nb">file</span><span class="p">)</span>
        <span class="n">header</span> <span class="o">=</span> <span class="nf">next</span><span class="p">(</span><span class="n">reader</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="c1"># CSV file does not exist, create and write the header
</span>    <span class="n">header</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">Episode</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Total Reward</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Epsilon</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Avg Reward (Last 100)</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Total Frames</span><span class="sh">"</span><span class="p">,</span>
              <span class="sh">"</span><span class="s">Frame Rate</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Model Updates</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Running Reward</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Training Time</span><span class="sh">"</span><span class="p">]</span>
    <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">csv_filename</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="sh">'</span><span class="s">w</span><span class="sh">'</span><span class="p">,</span> <span class="n">newline</span><span class="o">=</span><span class="sh">''</span><span class="p">)</span> <span class="k">as</span> <span class="nb">file</span><span class="p">:</span>
        <span class="n">writer</span> <span class="o">=</span> <span class="n">csv</span><span class="p">.</span><span class="nf">writer</span><span class="p">(</span><span class="nb">file</span><span class="p">)</span>
        <span class="n">writer</span><span class="p">.</span><span class="nf">writerow</span><span class="p">(</span><span class="n">header</span><span class="p">)</span>
</code></pre></div></div> <h3 id="how-the-training-loop-works">How the Training Loop Works</h3> <p>Now, let’s break down how the training loop operates. At each timestep, the agent selects an action based on the epsilon-greedy strategy, takes a step in the environment, and stores this transition in memory. A random batch of 32 transitions is then sampled from this replay buffer to train the neural network.</p> <p>For each training sample ((s, a, r, s’)) in the mini-batch, the model is provided with a state (a stack of four frames). Using the next state and the Bellman equation, we derive the targets for our neural network. Specifically, if the next state is a terminal state (meaning the episode has ended), the target is simply the immediate reward (r). Otherwise, the Q-value should reflect both the immediate reward and the discounted value of the highest future Q-value.</p> <p>This approach ensures that when an episode concludes, the next state is not considered in the update, which is facilitated by the <code class="language-plaintext highlighter-rouge">done_sample</code> array. By using the target network, we stabilize the learning process and avoid oscillations that could lead to divergence.</p> <p>Here’s how this training loop is implemented in Python:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">starting</span> <span class="o">=</span> <span class="n">datetime</span><span class="p">.</span><span class="n">datetime</span><span class="p">.</span><span class="nf">now</span><span class="p">()</span>
<span class="k">while</span> <span class="bp">True</span><span class="p">:</span>  <span class="c1"># Run until solved
</span>    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">())</span>

    <span class="n">current_lives</span> <span class="o">=</span> <span class="mi">5</span>

    <span class="n">episode_reward</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">timestep</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_steps_per_episode</span><span class="p">):</span>
        <span class="c1"># env.render(); Adding this line would show the attempts
</span>        <span class="c1"># of the agent in a pop up window.
</span>        <span class="n">frame_count</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c1"># Use epsilon-greedy for exploration
</span>        <span class="k">if</span> <span class="n">frame_count</span> <span class="o">&lt;</span> <span class="n">epsilon_random_frames</span> <span class="ow">or</span> <span class="n">epsilon</span> <span class="o">&gt;</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="c1"># Take random action
</span>            <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">num_actions</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Predict action Q-values
</span>            <span class="c1"># From environment state
</span>            <span class="n">state_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">convert_to_tensor</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
            <span class="n">state_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">expand_dims</span><span class="p">(</span><span class="n">state_tensor</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
            <span class="n">action_probs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">state_tensor</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
            <span class="c1"># Take the best action
</span>            <span class="n">action</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">action_probs</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="nf">numpy</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">frame_count</span> <span class="o">&gt;</span> <span class="n">epsilon_random_frames</span><span class="p">:</span> <span class="c1"># Decay epsilon only after exploring for first 50k frames
</span>            <span class="k">if</span> <span class="n">epsilon</span> <span class="o">&gt;</span> <span class="n">epsilon_min</span><span class="p">:</span>
                <span class="c1"># Decay probability of taking random action
</span>                <span class="n">epsilon</span> <span class="o">-=</span> <span class="n">epsilon_interval</span> <span class="o">/</span> <span class="n">epsilon_greedy_frames</span>
                <span class="n">epsilon</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">epsilon</span><span class="p">,</span> <span class="n">epsilon_min</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Continue decaying epsilon linearly over the remaining frames
</span>                <span class="n">epsilon</span> <span class="o">-=</span> <span class="n">epsilon_interval_2</span> <span class="o">/</span> <span class="p">(</span><span class="n">epsilon_final_frames</span><span class="p">)</span>
                <span class="n">epsilon</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">epsilon</span><span class="p">,</span> <span class="n">epsilon_final</span><span class="p">)</span>


        <span class="c1"># Apply the sampled action in our environment
</span>        <span class="n">state_next</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">state_next</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">state_next</span><span class="p">)</span>

        <span class="n">episode_reward</span> <span class="o">+=</span> <span class="n">reward</span>

        <span class="c1"># When a life is lost, we save terminal_life_lost = True in the replay memory
</span>        <span class="c1"># N.B. We don't modify directly done, since done is already used to break the loop
</span>        <span class="n">num_lives</span> <span class="o">=</span> <span class="n">info</span><span class="p">[</span><span class="sh">'</span><span class="s">lives</span><span class="sh">'</span><span class="p">]</span>

        <span class="nf">if </span><span class="p">(</span><span class="n">num_lives</span> <span class="o">&lt;</span> <span class="n">current_lives</span><span class="p">):</span>
            <span class="n">terminal_life_lost</span> <span class="o">=</span> <span class="bp">True</span>
            <span class="n">current_lives</span> <span class="o">=</span> <span class="n">num_lives</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">terminal_life_lost</span> <span class="o">=</span> <span class="bp">False</span>

        <span class="c1"># Save actions and states in replay buffer
</span>        <span class="n">action_history</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">state_history</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">state_next_history</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">state_next</span><span class="p">)</span>
        <span class="n">done_history</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">terminal_life_lost</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">done</span> <span class="k">else</span> <span class="n">done</span><span class="p">)</span> <span class="c1"># If the game is not terminated, if life lost add true, else add done (False or true)
</span>        <span class="n">rewards_history</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">state_next</span>

        <span class="c1"># Update every fourth frame and once batch size is over 32
</span>        <span class="k">if</span> <span class="n">frame_count</span> <span class="o">%</span> <span class="n">update_after_actions</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nf">len</span><span class="p">(</span><span class="n">done_history</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">batch_size</span><span class="p">:</span>

            <span class="c1"># Get indices of samples for replay buffers
</span>            <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">done_history</span><span class="p">)),</span> <span class="n">size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>

            <span class="c1"># Using list comprehension to sample from replay buffer
</span>            <span class="n">state_sample</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">state_history</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">])</span>
            <span class="n">state_next_sample</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">state_next_history</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">])</span>
            <span class="n">rewards_sample</span> <span class="o">=</span> <span class="p">[</span><span class="n">rewards_history</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">]</span>
            <span class="n">action_sample</span> <span class="o">=</span> <span class="p">[</span><span class="n">action_history</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">]</span>
            <span class="n">done_sample</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">convert_to_tensor</span><span class="p">(</span>
                <span class="p">[</span><span class="nf">float</span><span class="p">(</span><span class="n">done_history</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">]</span>
            <span class="p">)</span> <span class="c1"># turns True into 1.0 and False into 0.0.
</span>
            <span class="c1"># Build the updated Q-values for the sampled future states
</span>            <span class="c1"># Use the target model for stability
</span>            <span class="n">future_rewards</span> <span class="o">=</span> <span class="n">model_target</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">state_next_sample</span><span class="p">)</span>
            <span class="c1"># Q value = reward + discount factor * expected future reward
</span>            <span class="c1"># updated_q_values = rewards_sample + gamma * tf.reduce_max(
</span>            <span class="c1">#    future_rewards, axis=1
</span>            <span class="c1"># )
</span>
            <span class="c1"># Our Implementation
</span>            <span class="c1"># If the game is over because the agent lost or won, there is no next state and the value is simply the reward
</span>
            <span class="n">updated_q_values</span> <span class="o">=</span> <span class="n">rewards_sample</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">done_sample</span><span class="p">)</span> <span class="o">*</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_max</span><span class="p">(</span><span class="n">future_rewards</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># Create a mask so we only calculate loss on the updated Q-values (If action taken was 1, it create [0,1,0,0])
</span>            <span class="n">masks</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">one_hot</span><span class="p">(</span><span class="n">action_sample</span><span class="p">,</span> <span class="n">num_actions</span><span class="p">)</span>

            <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nc">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
                <span class="c1"># Train the model on the states and updated Q-values
</span>                <span class="n">q_values</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">state_sample</span><span class="p">)</span>

                <span class="c1">#  to the Q-values to get the Q-value for action taken
</span>                <span class="n">q_action</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">multiply</span><span class="p">(</span><span class="n">q_values</span><span class="p">,</span> <span class="n">masks</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="c1"># Calculate loss between new Q-value and old Q-value
</span>                <span class="n">loss</span> <span class="o">=</span> <span class="nf">loss_function</span><span class="p">(</span><span class="n">updated_q_values</span><span class="p">,</span> <span class="n">q_action</span><span class="p">)</span>

            <span class="c1"># Backpropagation
</span>            <span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="p">.</span><span class="nf">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="n">trainable_variables</span><span class="p">)</span>
            <span class="n">optimizer</span><span class="p">.</span><span class="nf">apply_gradients</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="n">trainable_variables</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">frame_count</span> <span class="o">%</span> <span class="n">update_target_network</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># update the the target network with new weights
</span>            <span class="n">model_target</span><span class="p">.</span><span class="nf">set_weights</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">get_weights</span><span class="p">())</span>
            <span class="c1"># Log details
</span>            <span class="n">template</span> <span class="o">=</span> <span class="sh">"</span><span class="s">running reward: {:.2f} at episode {}, frame count {}</span><span class="sh">"</span>
            <span class="nf">print</span><span class="p">(</span><span class="n">template</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">running_reward</span><span class="p">,</span> <span class="n">episode_count</span><span class="p">,</span> <span class="n">frame_count</span><span class="p">))</span>

        <span class="c1"># Limit the state and reward history
</span>        <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">rewards_history</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">max_memory_length</span><span class="p">:</span>
            <span class="k">del</span> <span class="n">rewards_history</span><span class="p">[:</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">del</span> <span class="n">state_history</span><span class="p">[:</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">del</span> <span class="n">state_next_history</span><span class="p">[:</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">del</span> <span class="n">action_history</span><span class="p">[:</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">del</span> <span class="n">done_history</span><span class="p">[:</span><span class="mi">1</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="c1"># print(info)
</span>            <span class="k">break</span>

    <span class="c1"># Update running reward to check condition for solving
</span>    <span class="n">episode_reward_history</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">episode_reward</span><span class="p">)</span>
    <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">episode_reward_history</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">100</span><span class="p">:</span>
        <span class="k">del</span> <span class="n">episode_reward_history</span><span class="p">[:</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">running_reward</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">episode_reward_history</span><span class="p">)</span>

    <span class="c1"># Calculate additional statistics
</span>    <span class="n">avg_reward_last_100</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">episode_reward_history</span><span class="p">[</span><span class="o">-</span><span class="mi">100</span><span class="p">:])</span>
    <span class="n">frame_rate</span> <span class="o">=</span> <span class="n">frame_count</span> <span class="o">/</span> <span class="p">(</span><span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">)</span>
    <span class="n">training_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>

    <span class="c1"># Append the episode statistics to the CSV file
</span>    <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">csv_filename</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="sh">'</span><span class="s">a</span><span class="sh">'</span><span class="p">,</span> <span class="n">newline</span><span class="o">=</span><span class="sh">''</span><span class="p">)</span> <span class="k">as</span> <span class="nb">file</span><span class="p">:</span>
        <span class="n">writer</span> <span class="o">=</span> <span class="n">csv</span><span class="p">.</span><span class="nf">writer</span><span class="p">(</span><span class="nb">file</span><span class="p">)</span>
        <span class="n">writer</span><span class="p">.</span><span class="nf">writerow</span><span class="p">([</span><span class="n">episode_count</span><span class="p">,</span> <span class="n">episode_reward</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">avg_reward_last_100</span><span class="p">,</span>
                            <span class="n">frame_count</span><span class="p">,</span> <span class="n">frame_rate</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">done_history</span><span class="p">),</span>
                            <span class="n">running_reward</span><span class="p">,</span> <span class="n">training_time</span><span class="p">])</span>

    <span class="nf">if </span><span class="p">(</span><span class="n">episode_count</span><span class="o">%</span><span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
        <span class="n">current_time</span> <span class="o">=</span> <span class="n">datetime</span><span class="p">.</span><span class="n">datetime</span><span class="p">.</span><span class="nf">now</span><span class="p">().</span><span class="nf">strftime</span><span class="p">(</span><span class="sh">"</span><span class="s">%Y-%m-%d %H:%M:%S</span><span class="sh">"</span><span class="p">)</span>

        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">current_time</span><span class="si">}</span><span class="s"> - Episode </span><span class="si">{</span><span class="n">episode_count</span><span class="si">}</span><span class="s"> reached. Saving model in saved_models/model_episode_</span><span class="si">{</span><span class="n">episode_count</span><span class="si">}</span><span class="s">. . .</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">model</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="sh">"</span><span class="s">saved_models/model_episode_{}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">episode_count</span><span class="p">))</span>
        <span class="n">current_time</span> <span class="o">=</span> <span class="n">datetime</span><span class="p">.</span><span class="n">datetime</span><span class="p">.</span><span class="nf">now</span><span class="p">().</span><span class="nf">strftime</span><span class="p">(</span><span class="sh">"</span><span class="s">%Y-%m-%d %H:%M:%S</span><span class="sh">"</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">current_time</span><span class="si">}</span><span class="s"> - Model saved.</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">current_time</span> <span class="o">=</span> <span class="n">datetime</span><span class="p">.</span><span class="n">datetime</span><span class="p">.</span><span class="nf">now</span><span class="p">().</span><span class="nf">strftime</span><span class="p">(</span><span class="sh">"</span><span class="s">%Y-%m-%d %H:%M:%S</span><span class="sh">"</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">current_time</span><span class="si">}</span><span class="s"> - Saving target model. . .</span><span class="sh">"</span><span class="p">)</span>
        <span class="c1"># Save the target model
</span>        <span class="n">model_target</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="sh">"</span><span class="s">saved_models/target_model_episode_{}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">episode_count</span><span class="p">))</span>
        <span class="n">current_time</span> <span class="o">=</span> <span class="n">datetime</span><span class="p">.</span><span class="n">datetime</span><span class="p">.</span><span class="nf">now</span><span class="p">().</span><span class="nf">strftime</span><span class="p">(</span><span class="sh">"</span><span class="s">%Y-%m-%d %H:%M:%S</span><span class="sh">"</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">current_time</span><span class="si">}</span><span class="s"> - Target model saved in saved_models/target_model_episode_</span><span class="si">{</span><span class="n">episode_count</span><span class="si">}</span><span class="s">.</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">episode_count</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="nf">if </span><span class="p">(</span><span class="n">num_lives</span><span class="o">==</span><span class="mi">0</span><span class="p">):</span>
        <span class="n">template</span> <span class="o">=</span> <span class="sh">"</span><span class="s">running reward: {:.2f} at episode {}, frame count {}</span><span class="sh">"</span>
        <span class="nf">print</span><span class="p">(</span><span class="n">template</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">running_reward</span><span class="p">,</span> <span class="n">episode_count</span><span class="p">,</span> <span class="n">frame_count</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">running_reward</span> <span class="o">&gt;</span> <span class="mi">40</span><span class="p">:</span>  <span class="c1"># 40 is the avg score of human beings
</span>        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Solved at episode {}!</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">episode_count</span><span class="p">))</span>
        <span class="n">episode_count</span> <span class="o">-=</span> <span class="mi">1</span>
        <span class="k">break</span>
</code></pre></div></div> <h2 id="conclusion">Conclusion</h2> <p>Getting Deep Q-Learning to work can be a bit of a pain, right? There are so many little details to tweak, and if you miss one, things just don’t click. Plus, if you’re running experiments on a single GPU (or even without), get ready for those overnight waits! Debugging becomes a slow grind, and it can feel like you’re just spinning your wheels sometimes.</p> <p>But honestly, it’s been totally worth it. I learned a ton about neural networks and reinforcement learning through all the debugging and fine-tuning. It’s amazing to see the progress your agent makes!</p> <p>So, give it a shot! Implement your own DQN, tweak it, and watch your AI tackle Atari Breakout. It’s a rewarding experience, and who knows? You might just create something impressive.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 d-flex align-items-center justify-content-center" style="height: 100%;"> <figure> <video src="/assets/video/ATARI_Breakout_Eval_model_21700_reward_357-speed.mp4" class="img-fluid rounded z-depth-1" width="200" height="auto" autoplay="" controls=""/> </figure> </div> </div> <p>If you have any questions or want to share your journey, feel free to contact me. Happy coding, and enjoy the ride!</p>]]></content><author><name></name></author><category term="tutorials"/><category term="DQN,"/><category term="Atari,"/><category term="Reinforcement"/><category term="Learning,"/><category term="Deep"/><category term="Learning"/><summary type="html"><![CDATA[An easy-to-follow guide on implementing a Deep Q-Network (DQN) for Atari Breakout.]]></summary></entry><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://francesco-fortunato.github.io/it/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://francesco-fortunato.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://francesco-fortunato.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[We’re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">a post with tabs</title><link href="https://francesco-fortunato.github.io/it/blog/2024/tabs/" rel="alternate" type="text/html" title="a post with tabs"/><published>2024-05-01T00:32:13+00:00</published><updated>2024-05-01T00:32:13+00:00</updated><id>https://francesco-fortunato.github.io/blog/2024/tabs</id><content type="html" xml:base="https://francesco-fortunato.github.io/blog/2024/tabs/"><![CDATA[<p>This is how a post with <a href="https://github.com/Ovski4/jekyll-tabs">tabs</a> looks like. Note that the tabs could be used for different purposes, not only for code.</p> <h2 id="first-tabs">First tabs</h2> <p>To add tabs, use the following syntax:</p> <div class="language-liquid highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">{%</span><span class="w"> </span><span class="nt">tabs</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-1</span><span class="w"> </span><span class="cp">%}</span>

Content 1

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-2</span><span class="w"> </span><span class="cp">%}</span>

Content 2

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtabs</span><span class="w"> </span><span class="cp">%}</span>
</code></pre></div></div> <p>With this you can generate visualizations like:</p> <ul id="log" class="tab" data-tab="cff91200-d13a-4cf7-a2af-43b7af9a7a69" data-name="log"> <li class="active" id="log-php"> <a href="#">php </a> </li> <li id="log-js"> <a href="#">js </a> </li> <li id="log-ruby"> <a href="#">ruby </a> </li> </ul> <ul class="tab-content" id="cff91200-d13a-4cf7-a2af-43b7af9a7a69" data-name="log"> <li class="active"> <div class="language-php highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">var_dump</span><span class="p">(</span><span class="s1">'hello'</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="dl">"</span><span class="s2">hello</span><span class="dl">"</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">pputs</span> <span class="dl">'</span><span class="s1">hello</span><span class="dl">'</span>
</code></pre></div></div> </li> </ul> <h2 id="another-example">Another example</h2> <ul id="data-struct" class="tab" data-tab="336d53d7-4de4-4f98-b0d5-54c889eb324a" data-name="data-struct"> <li class="active" id="data-struct-yaml"> <a href="#">yaml </a> </li> <li id="data-struct-json"> <a href="#">json </a> </li> </ul> <ul class="tab-content" id="336d53d7-4de4-4f98-b0d5-54c889eb324a" data-name="data-struct"> <li class="active"> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">hello</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">whatsup"</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">hi"</span>
</code></pre></div></div> </li> <li> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"hello"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"whatsup"</span><span class="p">,</span><span class="w"> </span><span class="s2">"hi"</span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> </li> </ul> <h2 id="tabs-for-something-else">Tabs for something else</h2> <ul id="something-else" class="tab" data-tab="5b43ab35-16c5-40c3-b317-8d54caa643d3" data-name="something-else"> <li class="active" id="something-else-text"> <a href="#">text </a> </li> <li id="something-else-quote"> <a href="#">quote </a> </li> <li id="something-else-list"> <a href="#">list </a> </li> </ul> <ul class="tab-content" id="5b43ab35-16c5-40c3-b317-8d54caa643d3" data-name="something-else"> <li class="active"> <p>Regular text</p> </li> <li> <blockquote> <p>A quote</p> </blockquote> </li> <li> <p>Hipster list</p> <ul> <li>brunch</li> <li>fixie</li> <li>raybans</li> <li>messenger bag</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included tabs in a post could look like]]></summary></entry><entry><title type="html">a post with typograms</title><link href="https://francesco-fortunato.github.io/it/blog/2024/typograms/" rel="alternate" type="text/html" title="a post with typograms"/><published>2024-04-29T23:36:10+00:00</published><updated>2024-04-29T23:36:10+00:00</updated><id>https://francesco-fortunato.github.io/blog/2024/typograms</id><content type="html" xml:base="https://francesco-fortunato.github.io/blog/2024/typograms/"><![CDATA[<p>This is an example post with some <a href="https://github.com/google/typograms/">typograms</a> code.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">typograms
</span><span class="sb">+----+
|    |---&gt; My first diagram!
+----+</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-typograms">+----+
|    |---&gt; My first diagram!
+----+
</code></pre> <p>Another example:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">typograms
</span><span class="sb">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.</span>
<span class="p">```</span>
</code></pre></div></div> <p>which generates:</p> <pre><code class="language-typograms">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.
</code></pre> <p>For more examples, check out the <a href="https://google.github.io/typograms/#examples">typograms documentation</a>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="diagrams"/><summary type="html"><![CDATA[this is what included typograms code could look like]]></summary></entry><entry><title type="html">a post that can be cited</title><link href="https://francesco-fortunato.github.io/it/blog/2024/post-citation/" rel="alternate" type="text/html" title="a post that can be cited"/><published>2024-04-28T15:06:00+00:00</published><updated>2024-04-28T15:06:00+00:00</updated><id>https://francesco-fortunato.github.io/blog/2024/post-citation</id><content type="html" xml:base="https://francesco-fortunato.github.io/blog/2024/post-citation/"><![CDATA[<p>This is an example post that can be cited. The content of the post ends here, while the citation information is automatically provided below. The only thing needed is for you to set the <code class="language-plaintext highlighter-rouge">citation</code> key in the front matter to <code class="language-plaintext highlighter-rouge">true</code>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="citation"/><summary type="html"><![CDATA[this is what a post that can be cited looks like]]></summary></entry><entry><title type="html">a post with pseudo code</title><link href="https://francesco-fortunato.github.io/it/blog/2024/pseudocode/" rel="alternate" type="text/html" title="a post with pseudo code"/><published>2024-04-15T00:01:00+00:00</published><updated>2024-04-15T00:01:00+00:00</updated><id>https://francesco-fortunato.github.io/blog/2024/pseudocode</id><content type="html" xml:base="https://francesco-fortunato.github.io/blog/2024/pseudocode/"><![CDATA[<p>This is an example post with some pseudo code rendered by <a href="https://github.com/SaswatPadhi/pseudocode.js">pseudocode</a>. The example presented here is the same as the one in the <a href="https://saswat.padhi.me/pseudocode.js/">pseudocode.js</a> documentation, with only one simple but important change: everytime you would use <code class="language-plaintext highlighter-rouge">$</code>, you should use <code class="language-plaintext highlighter-rouge">$$</code> instead. Also, note that the <code class="language-plaintext highlighter-rouge">pseudocode</code> key in the front matter is set to <code class="language-plaintext highlighter-rouge">true</code> to enable the rendering of pseudo code. As an example, using this code:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">pseudocode
</span><span class="sb">% This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)
\begin{algorithm}
\caption{Quicksort}
\begin{algorithmic}
\PROCEDURE{Quicksort}{$$A, p, r$$}
    \IF{$$p &lt; r$$}
        \STATE $$q = $$ \CALL{Partition}{$$A, p, r$$}
        \STATE \CALL{Quicksort}{$$A, p, q - 1$$}
        \STATE \CALL{Quicksort}{$$A, q + 1, r$$}
    \ENDIF
\ENDPROCEDURE
\PROCEDURE{Partition}{$$A, p, r$$}
    \STATE $$x = A[r]$$
    \STATE $$i = p - 1$$
    \FOR{$$j = p$$ \TO $$r - 1$$}
        \IF{$$A[j] &lt; x$$}
            \STATE $$i = i + 1$$
            \STATE exchange
            $$A[i]$$ with $$A[j]$$
        \ENDIF
        \STATE exchange $$A[i]$$ with $$A[r]$$
    \ENDFOR
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}</span>
<span class="p">```</span>
</code></pre></div></div> <p>Generates:</p> <pre><code class="language-pseudocode">% This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)
\begin{algorithm}
\caption{Quicksort}
\begin{algorithmic}
\PROCEDURE{Quicksort}{$$A, p, r$$}
    \IF{$$p &lt; r$$}
        \STATE $$q = $$ \CALL{Partition}{$$A, p, r$$}
        \STATE \CALL{Quicksort}{$$A, p, q - 1$$}
        \STATE \CALL{Quicksort}{$$A, q + 1, r$$}
    \ENDIF
\ENDPROCEDURE
\PROCEDURE{Partition}{$$A, p, r$$}
    \STATE $$x = A[r]$$
    \STATE $$i = p - 1$$
    \FOR{$$j = p$$ \TO $$r - 1$$}
        \IF{$$A[j] &lt; x$$}
            \STATE $$i = i + 1$$
            \STATE exchange
            $$A[i]$$ with $$A[j]$$
        \ENDIF
        \STATE exchange $$A[i]$$ with $$A[r]$$
    \ENDFOR
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}
</code></pre>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included pseudo code could look like]]></summary></entry><entry><title type="html">a post with code diff</title><link href="https://francesco-fortunato.github.io/it/blog/2024/code-diff/" rel="alternate" type="text/html" title="a post with code diff"/><published>2024-01-27T19:22:00+00:00</published><updated>2024-01-27T19:22:00+00:00</updated><id>https://francesco-fortunato.github.io/blog/2024/code-diff</id><content type="html" xml:base="https://francesco-fortunato.github.io/blog/2024/code-diff/"><![CDATA[<p>You can display diff code by using the regular markdown syntax:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">diff
</span><span class="gh">diff --git a/sample.js b/sample.js
index 0000001..0ddf2ba
</span><span class="gd">--- a/sample.js
</span><span class="gi">+++ b/sample.js
</span><span class="p">@@ -1 +1 @@</span>
<span class="gd">-console.log("Hello World!")
</span><span class="gi">+console.log("Hello from Diff2Html!")</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gh">diff --git a/sample.js b/sample.js
index 0000001..0ddf2ba
</span><span class="gd">--- a/sample.js
</span><span class="gi">+++ b/sample.js
</span><span class="p">@@ -1 +1 @@</span>
<span class="gd">-console.log("Hello World!")
</span><span class="gi">+console.log("Hello from Diff2Html!")
</span></code></pre></div></div> <p>But this is difficult to read, specially if you have a large diff. You can use <a href="https://diff2html.xyz/">diff2html</a> to display a more readable version of the diff. For this, just use <code class="language-plaintext highlighter-rouge">diff2html</code> instead of <code class="language-plaintext highlighter-rouge">diff</code> for the code block language:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">diff2html
</span><span class="sb">diff --git a/sample.js b/sample.js
index 0000001..0ddf2ba
--- a/sample.js
+++ b/sample.js
@@ -1 +1 @@
-console.log("Hello World!")
+console.log("Hello from Diff2Html!")</span>
<span class="p">```</span>
</code></pre></div></div> <p>If we use a longer example, for example <a href="https://github.com/rtfpessoa/diff2html/commit/c2c253d3e3f8b8b267f551e659f72b44ca2ac927">this commit from diff2html</a>, it will generate the following output:</p> <pre><code class="language-diff2html">From 2aaae31cc2a37bfff83430c2c914b140bee59b6a Mon Sep 17 00:00:00 2001
From: Rodrigo Fernandes &lt;rtfrodrigo@gmail.com&gt;
Date: Sun, 9 Oct 2016 16:41:54 +0100
Subject: [PATCH 1/2] Initial template override support

---
 scripts/hulk.js                    |  4 ++--
 src/diff2html.js                   |  3 +--
 src/file-list-printer.js           | 11 ++++++++---
 src/hoganjs-utils.js               | 29 +++++++++++++++++------------
 src/html-printer.js                |  6 ++++++
 src/line-by-line-printer.js        |  6 +++++-
 src/side-by-side-printer.js        |  6 +++++-
 test/file-list-printer-tests.js    |  2 +-
 test/hogan-cache-tests.js          | 18 +++++++++++++++---
 test/line-by-line-tests.js         |  3 +--
 test/side-by-side-printer-tests.js |  3 +--
 11 files changed, 62 insertions(+), 29 deletions(-)

diff --git a/scripts/hulk.js b/scripts/hulk.js
index 5a793c18..a4b1a4d5 100755
--- a/scripts/hulk.js
+++ b/scripts/hulk.js
@@ -173,11 +173,11 @@ function namespace(name) {
 // write a template foreach file that matches template extension
 templates = extractFiles(options.argv.remain)
   .map(function(file) {
-    var openedFile = fs.readFileSync(file, 'utf-8');
+    var openedFile = fs.readFileSync(file, 'utf-8').trim();
     var name;
     if (!openedFile) return;
     name = namespace(path.basename(file).replace(/\..*$/, ''));
-    openedFile = removeByteOrderMark(openedFile.trim());
+    openedFile = removeByteOrderMark(openedFile);
     openedFile = wrap(file, name, openedFile);
     if (!options.outputdir) return openedFile;
     fs.writeFileSync(path.join(options.outputdir, name + '.js')
diff --git a/src/diff2html.js b/src/diff2html.js
index 21b0119e..64e138f5 100644
--- a/src/diff2html.js
+++ b/src/diff2html.js
@@ -7,7 +7,6 @@

 (function() {
   var diffParser = require('./diff-parser.js').DiffParser;
-  var fileLister = require('./file-list-printer.js').FileListPrinter;
   var htmlPrinter = require('./html-printer.js').HtmlPrinter;

   function Diff2Html() {
@@ -43,7 +42,7 @@

     var fileList = '';
     if (configOrEmpty.showFiles === true) {
-      fileList = fileLister.generateFileList(diffJson, configOrEmpty);
+      fileList = htmlPrinter.generateFileListSummary(diffJson, configOrEmpty);
     }

     var diffOutput = '';
diff --git a/src/file-list-printer.js b/src/file-list-printer.js
index e408d9b2..1e0a2c61 100644
--- a/src/file-list-printer.js
+++ b/src/file-list-printer.js
@@ -8,11 +8,16 @@
 (function() {
   var printerUtils = require('./printer-utils.js').PrinterUtils;

-  var hoganUtils = require('./hoganjs-utils.js').HoganJsUtils;
+  var hoganUtils;
+
   var baseTemplatesPath = 'file-summary';
   var iconsBaseTemplatesPath = 'icon';

-  function FileListPrinter() {
+  function FileListPrinter(config) {
+    this.config = config;
+
+    var HoganJsUtils = require('./hoganjs-utils.js').HoganJsUtils;
+    hoganUtils = new HoganJsUtils(config);
   }

   FileListPrinter.prototype.generateFileList = function(diffFiles) {
@@ -38,5 +43,5 @@
     });
   };

-  module.exports.FileListPrinter = new FileListPrinter();
+  module.exports.FileListPrinter = FileListPrinter;
 })();
diff --git a/src/hoganjs-utils.js b/src/hoganjs-utils.js
index 9949e5fa..0dda08d7 100644
--- a/src/hoganjs-utils.js
+++ b/src/hoganjs-utils.js
@@ -8,18 +8,19 @@
 (function() {
   var fs = require('fs');
   var path = require('path');
-
   var hogan = require('hogan.js');

   var hoganTemplates = require('./templates/diff2html-templates.js');

-  var templatesPath = path.resolve(__dirname, 'templates');
+  var extraTemplates;

-  function HoganJsUtils() {
+  function HoganJsUtils(configuration) {
+    this.config = configuration || {};
+    extraTemplates = this.config.templates || {};
   }

-  HoganJsUtils.prototype.render = function(namespace, view, params, configuration) {
-    var template = this.template(namespace, view, configuration);
+  HoganJsUtils.prototype.render = function(namespace, view, params) {
+    var template = this.template(namespace, view);
     if (template) {
       return template.render(params);
     }
@@ -27,17 +28,16 @@
     return null;
   };

-  HoganJsUtils.prototype.template = function(namespace, view, configuration) {
-    var config = configuration || {};
+  HoganJsUtils.prototype.template = function(namespace, view) {
     var templateKey = this._templateKey(namespace, view);

-    return this._getTemplate(templateKey, config);
+    return this._getTemplate(templateKey);
   };

-  HoganJsUtils.prototype._getTemplate = function(templateKey, config) {
+  HoganJsUtils.prototype._getTemplate = function(templateKey) {
     var template;

-    if (!config.noCache) {
+    if (!this.config.noCache) {
       template = this._readFromCache(templateKey);
     }

@@ -53,6 +53,7 @@

     try {
       if (fs.readFileSync) {
+        var templatesPath = path.resolve(__dirname, 'templates');
         var templatePath = path.join(templatesPath, templateKey);
         var templateContent = fs.readFileSync(templatePath + '.mustache', 'utf8');
         template = hogan.compile(templateContent);
@@ -66,12 +67,16 @@
   };

   HoganJsUtils.prototype._readFromCache = function(templateKey) {
-    return hoganTemplates[templateKey];
+    return extraTemplates[templateKey] || hoganTemplates[templateKey];
   };

   HoganJsUtils.prototype._templateKey = function(namespace, view) {
     return namespace + '-' + view;
   };

-  module.exports.HoganJsUtils = new HoganJsUtils();
+  HoganJsUtils.prototype.compile = function(templateStr) {
+    return hogan.compile(templateStr);
+  };
+
+  module.exports.HoganJsUtils = HoganJsUtils;
 })();
diff --git a/src/html-printer.js b/src/html-printer.js
index 585d5b66..13f83047 100644
--- a/src/html-printer.js
+++ b/src/html-printer.js
@@ -8,6 +8,7 @@
 (function() {
   var LineByLinePrinter = require('./line-by-line-printer.js').LineByLinePrinter;
   var SideBySidePrinter = require('./side-by-side-printer.js').SideBySidePrinter;
+  var FileListPrinter = require('./file-list-printer.js').FileListPrinter;

   function HtmlPrinter() {
   }
@@ -22,5 +23,10 @@
     return sideBySidePrinter.generateSideBySideJsonHtml(diffFiles);
   };

+  HtmlPrinter.prototype.generateFileListSummary = function(diffJson, config) {
+    var fileListPrinter = new FileListPrinter(config);
+    return fileListPrinter.generateFileList(diffJson);
+  };
+
   module.exports.HtmlPrinter = new HtmlPrinter();
 })();
diff --git a/src/line-by-line-printer.js b/src/line-by-line-printer.js
index b07eb53c..d230bedd 100644
--- a/src/line-by-line-printer.js
+++ b/src/line-by-line-printer.js
@@ -11,7 +11,8 @@
   var utils = require('./utils.js').Utils;
   var Rematch = require('./rematch.js').Rematch;

-  var hoganUtils = require('./hoganjs-utils.js').HoganJsUtils;
+  var hoganUtils;
+
   var genericTemplatesPath = 'generic';
   var baseTemplatesPath = 'line-by-line';
   var iconsBaseTemplatesPath = 'icon';
@@ -19,6 +20,9 @@

   function LineByLinePrinter(config) {
     this.config = config;
+
+    var HoganJsUtils = require('./hoganjs-utils.js').HoganJsUtils;
+    hoganUtils = new HoganJsUtils(config);
   }

   LineByLinePrinter.prototype.makeFileDiffHtml = function(file, diffs) {
diff --git a/src/side-by-side-printer.js b/src/side-by-side-printer.js
index bbf1dc8d..5e3033b3 100644
--- a/src/side-by-side-printer.js
+++ b/src/side-by-side-printer.js
@@ -11,7 +11,8 @@
   var utils = require('./utils.js').Utils;
   var Rematch = require('./rematch.js').Rematch;

-  var hoganUtils = require('./hoganjs-utils.js').HoganJsUtils;
+  var hoganUtils;
+
   var genericTemplatesPath = 'generic';
   var baseTemplatesPath = 'side-by-side';
   var iconsBaseTemplatesPath = 'icon';
@@ -26,6 +27,9 @@

   function SideBySidePrinter(config) {
     this.config = config;
+
+    var HoganJsUtils = require('./hoganjs-utils.js').HoganJsUtils;
+    hoganUtils = new HoganJsUtils(config);
   }

   SideBySidePrinter.prototype.makeDiffHtml = function(file, diffs) {
diff --git a/test/file-list-printer-tests.js b/test/file-list-printer-tests.js
index a502a46f..60ea3208 100644
--- a/test/file-list-printer-tests.js
+++ b/test/file-list-printer-tests.js
@@ -1,6 +1,6 @@
 var assert = require('assert');

-var fileListPrinter = require('../src/file-list-printer.js').FileListPrinter;
+var fileListPrinter = new (require('../src/file-list-printer.js').FileListPrinter)();

 describe('FileListPrinter', function() {
   describe('generateFileList', function() {
diff --git a/test/hogan-cache-tests.js b/test/hogan-cache-tests.js
index 190bf6f8..3bb754ac 100644
--- a/test/hogan-cache-tests.js
+++ b/test/hogan-cache-tests.js
@@ -1,6 +1,6 @@
 var assert = require('assert');

-var HoganJsUtils = require('../src/hoganjs-utils.js').HoganJsUtils;
+var HoganJsUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)();
 var diffParser = require('../src/diff-parser.js').DiffParser;

 describe('HoganJsUtils', function() {
@@ -21,16 +21,28 @@ describe('HoganJsUtils', function() {
       });
       assert.equal(emptyDiffHtml, result);
     });
+
     it('should render view without cache', function() {
       var result = HoganJsUtils.render('generic', 'empty-diff', {
         contentClass: 'd2h-code-line',
         diffParser: diffParser
       }, {noCache: true});
-      assert.equal(emptyDiffHtml + '\n', result);
+      assert.equal(emptyDiffHtml, result);
     });
+
     it('should return null if template is missing', function() {
-      var result = HoganJsUtils.render('generic', 'missing-template', {}, {noCache: true});
+      var hoganUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)({noCache: true});
+      var result = hoganUtils.render('generic', 'missing-template', {});
       assert.equal(null, result);
     });
+
+    it('should allow templates to be overridden', function() {
+      var emptyDiffTemplate = HoganJsUtils.compile('&lt;p&gt;&lt;/p&gt;');
+
+      var config = {templates: {'generic-empty-diff': emptyDiffTemplate}};
+      var hoganUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)(config);
+      var result = hoganUtils.render('generic', 'empty-diff', {myName: 'Rodrigo Fernandes'});
+      assert.equal('&lt;p&gt;Rodrigo Fernandes&lt;/p&gt;', result);
+    });
   });
 });
diff --git a/test/line-by-line-tests.js b/test/line-by-line-tests.js
index 1cd92073..8869b3df 100644
--- a/test/line-by-line-tests.js
+++ b/test/line-by-line-tests.js
@@ -14,7 +14,7 @@ describe('LineByLinePrinter', function() {
         '            File without changes\n' +
         '        &lt;/div&gt;\n' +
         '    &lt;/td&gt;\n' +
-        '&lt;/tr&gt;\n';
+        '&lt;/tr&gt;';

       assert.equal(expected, fileHtml);
     });
@@ -422,7 +422,6 @@ describe('LineByLinePrinter', function() {
         '        &lt;/div&gt;\n' +
         '    &lt;/td&gt;\n' +
         '&lt;/tr&gt;\n' +
-        '\n' +
         '                &lt;/tbody&gt;\n' +
         '            &lt;/table&gt;\n' +
         '        &lt;/div&gt;\n' +
diff --git a/test/side-by-side-printer-tests.js b/test/side-by-side-printer-tests.js
index 76625f8e..771daaa5 100644
--- a/test/side-by-side-printer-tests.js
+++ b/test/side-by-side-printer-tests.js
@@ -14,7 +14,7 @@ describe('SideBySidePrinter', function() {
         '            File without changes\n' +
         '        &lt;/div&gt;\n' +
         '    &lt;/td&gt;\n' +
-        '&lt;/tr&gt;\n';
+        '&lt;/tr&gt;';

       assert.equal(expectedRight, fileHtml.right);
       assert.equal(expectedLeft, fileHtml.left);
@@ -324,7 +324,6 @@ describe('SideBySidePrinter', function() {
         '        &lt;/div&gt;\n' +
         '    &lt;/td&gt;\n' +
         '&lt;/tr&gt;\n' +
-        '\n' +
         '                    &lt;/tbody&gt;\n' +
         '                &lt;/table&gt;\n' +
         '            &lt;/div&gt;\n' +

From f3cadb96677d0eb82fc2752dc3ffbf35ca9b5bdb Mon Sep 17 00:00:00 2001
From: Rodrigo Fernandes &lt;rtfrodrigo@gmail.com&gt;
Date: Sat, 15 Oct 2016 13:21:22 +0100
Subject: [PATCH 2/2] Allow uncompiled templates

---
 README.md                 |  3 +++
 src/hoganjs-utils.js      |  7 +++++++
 test/hogan-cache-tests.js | 24 +++++++++++++++++++++++-
 3 files changed, 33 insertions(+), 1 deletion(-)

diff --git a/README.md b/README.md
index 132c8a28..46909f25 100644
--- a/README.md
+++ b/README.md
@@ -98,6 +98,9 @@ The HTML output accepts a Javascript object with configuration. Possible options
   - `synchronisedScroll`: scroll both panes in side-by-side mode: `true` or `false`, default is `false`
   - `matchWordsThreshold`: similarity threshold for word matching, default is 0.25
   - `matchingMaxComparisons`: perform at most this much comparisons for line matching a block of changes, default is `2500`
+  - `templates`: object with previously compiled templates to replace parts of the html
+  - `rawTemplates`: object with raw not compiled templates to replace parts of the html
+  &gt; For more information regarding the possible templates look into [src/templates](https://github.com/rtfpessoa/diff2html/tree/master/src/templates)

 ## Diff2HtmlUI Helper

diff --git a/src/hoganjs-utils.js b/src/hoganjs-utils.js
index 0dda08d7..b2e9c275 100644
--- a/src/hoganjs-utils.js
+++ b/src/hoganjs-utils.js
@@ -17,6 +17,13 @@
   function HoganJsUtils(configuration) {
     this.config = configuration || {};
     extraTemplates = this.config.templates || {};
+
+    var rawTemplates = this.config.rawTemplates || {};
+    for (var templateName in rawTemplates) {
+      if (rawTemplates.hasOwnProperty(templateName)) {
+        if (!extraTemplates[templateName]) extraTemplates[templateName] = this.compile(rawTemplates[templateName]);
+      }
+    }
   }

   HoganJsUtils.prototype.render = function(namespace, view, params) {
diff --git a/test/hogan-cache-tests.js b/test/hogan-cache-tests.js
index 3bb754ac..a34839c0 100644
--- a/test/hogan-cache-tests.js
+++ b/test/hogan-cache-tests.js
@@ -36,7 +36,7 @@ describe('HoganJsUtils', function() {
       assert.equal(null, result);
     });

-    it('should allow templates to be overridden', function() {
+    it('should allow templates to be overridden with compiled templates', function() {
       var emptyDiffTemplate = HoganJsUtils.compile('&lt;p&gt;&lt;/p&gt;');

       var config = {templates: {'generic-empty-diff': emptyDiffTemplate}};
@@ -44,5 +44,27 @@ describe('HoganJsUtils', function() {
       var result = hoganUtils.render('generic', 'empty-diff', {myName: 'Rodrigo Fernandes'});
       assert.equal('&lt;p&gt;Rodrigo Fernandes&lt;/p&gt;', result);
     });
+
+    it('should allow templates to be overridden with uncompiled templates', function() {
+      var emptyDiffTemplate = '&lt;p&gt;&lt;/p&gt;';
+
+      var config = {rawTemplates: {'generic-empty-diff': emptyDiffTemplate}};
+      var hoganUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)(config);
+      var result = hoganUtils.render('generic', 'empty-diff', {myName: 'Rodrigo Fernandes'});
+      assert.equal('&lt;p&gt;Rodrigo Fernandes&lt;/p&gt;', result);
+    });
+
+    it('should allow templates to be overridden giving priority to compiled templates', function() {
+      var emptyDiffTemplate = HoganJsUtils.compile('&lt;p&gt;&lt;/p&gt;');
+      var emptyDiffTemplateUncompiled = '&lt;p&gt;Not used!&lt;/p&gt;';
+
+      var config = {
+        templates: {'generic-empty-diff': emptyDiffTemplate},
+        rawTemplates: {'generic-empty-diff': emptyDiffTemplateUncompiled}
+      };
+      var hoganUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)(config);
+      var result = hoganUtils.render('generic', 'empty-diff', {myName: 'Rodrigo Fernandes'});
+      assert.equal('&lt;p&gt;Rodrigo Fernandes&lt;/p&gt;', result);
+    });
   });
 });
</code></pre>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is how you can display code diffs]]></summary></entry><entry><title type="html">a post with advanced image components</title><link href="https://francesco-fortunato.github.io/it/blog/2024/advanced-images/" rel="alternate" type="text/html" title="a post with advanced image components"/><published>2024-01-27T11:46:00+00:00</published><updated>2024-01-27T11:46:00+00:00</updated><id>https://francesco-fortunato.github.io/blog/2024/advanced-images</id><content type="html" xml:base="https://francesco-fortunato.github.io/blog/2024/advanced-images/"><![CDATA[<p>This is an example post with advanced image components.</p> <h2 id="image-slider">Image Slider</h2> <p>This is a simple image slider. It uses the <a href="https://swiperjs.com/">Swiper</a> library. Check the <a href="https://swiperjs.com/demos">examples page</a> for more information of what you can achieve with it.</p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/9-480.webp 480w,/assets/img/9-800.webp 800w,/assets/img/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/9.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/7-480.webp 480w,/assets/img/7-800.webp 800w,/assets/img/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/7.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/8-480.webp 480w,/assets/img/8-800.webp 800w,/assets/img/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/8.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/10-480.webp 480w,/assets/img/10-800.webp 800w,/assets/img/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/10.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/12-480.webp 480w,/assets/img/12-800.webp 800w,/assets/img/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/12.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container> <h2 id="image-comparison-slider">Image Comparison Slider</h2> <p>This is a simple image comparison slider. It uses the <a href="https://img-comparison-slider.sneas.io/">img-comparison-slider</a> library. Check the <a href="https://img-comparison-slider.sneas.io/examples.html">examples page</a> for more information of what you can achieve with it.</p> <img-comparison-slider> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic-480.webp 480w,/assets/img/prof_pic-800.webp 800w,/assets/img/prof_pic-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/prof_pic.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic_color-480.webp 480w,/assets/img/prof_pic_color-800.webp 800w,/assets/img/prof_pic_color-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/prof_pic_color.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </img-comparison-slider>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[this is what advanced image components could look like]]></summary></entry><entry><title type="html">a post with vega lite</title><link href="https://francesco-fortunato.github.io/it/blog/2024/vega-lite/" rel="alternate" type="text/html" title="a post with vega lite"/><published>2024-01-27T00:20:00+00:00</published><updated>2024-01-27T00:20:00+00:00</updated><id>https://francesco-fortunato.github.io/blog/2024/vega-lite</id><content type="html" xml:base="https://francesco-fortunato.github.io/blog/2024/vega-lite/"><![CDATA[<p>This is an example post with some <a href="https://vega.github.io/vega-lite/">vega lite</a> code.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">vega_lite
</span><span class="sb">{
  "$schema": "https://vega.github.io/schema/vega-lite/v5.json",
  "description": "A dot plot showing each movie in the database, and the difference from the average movie rating. The display is sorted by year to visualize everything in sequential order. The graph is for all Movies before 2019.",
  "data": {
    "url": "https://raw.githubusercontent.com/vega/vega/main/docs/data/movies.json"
  },
  "transform": [
    {"filter": "datum['IMDB Rating'] != null"},
    {"filter": {"timeUnit": "year", "field": "Release Date", "range": [null, 2019]}},
    {
      "joinaggregate": [{
        "op": "mean",
        "field": "IMDB Rating",
        "as": "AverageRating"
      }]
    },
    {
      "calculate": "datum['IMDB Rating'] - datum.AverageRating",
      "as": "RatingDelta"
    }
  ],
  "mark": "point",
  "encoding": {
    "x": {
      "field": "Release Date",
      "type": "temporal"
    },
    "y": {
      "field": "RatingDelta",
      "type": "quantitative",
      "title": "Rating Delta"
    },
    "color": {
      "field": "RatingDelta",
      "type": "quantitative",
      "scale": {"domainMid": 0},
      "title": "Rating Delta"
    }
  }
}</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-vega_lite">{
  "$schema": "https://vega.github.io/schema/vega-lite/v5.json",
  "description": "A dot plot showing each movie in the database, and the difference from the average movie rating. The display is sorted by year to visualize everything in sequential order. The graph is for all Movies before 2019.",
  "data": {
    "url": "https://raw.githubusercontent.com/vega/vega/main/docs/data/movies.json"
  },
  "transform": [
    {"filter": "datum['IMDB Rating'] != null"},
    {"filter": {"timeUnit": "year", "field": "Release Date", "range": [null, 2019]}},
    {
      "joinaggregate": [{
        "op": "mean",
        "field": "IMDB Rating",
        "as": "AverageRating"
      }]
    },
    {
      "calculate": "datum['IMDB Rating'] - datum.AverageRating",
      "as": "RatingDelta"
    }
  ],
  "mark": "point",
  "encoding": {
    "x": {
      "field": "Release Date",
      "type": "temporal"
    },
    "y": {
      "field": "RatingDelta",
      "type": "quantitative",
      "title": "Rating Delta"
    },
    "color": {
      "field": "RatingDelta",
      "type": "quantitative",
      "scale": {"domainMid": 0},
      "title": "Rating Delta"
    }
  }
}
</code></pre> <p>This plot supports both light and dark themes.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="charts"/><summary type="html"><![CDATA[this is what included vega lite code could look like]]></summary></entry><entry><title type="html">a post with geojson</title><link href="https://francesco-fortunato.github.io/it/blog/2024/geojson-map/" rel="alternate" type="text/html" title="a post with geojson"/><published>2024-01-26T17:57:00+00:00</published><updated>2024-01-26T17:57:00+00:00</updated><id>https://francesco-fortunato.github.io/blog/2024/geojson-map</id><content type="html" xml:base="https://francesco-fortunato.github.io/blog/2024/geojson-map/"><![CDATA[<p>This is an example post with some <a href="https://geojson.org/">geojson</a> code. The support is provided thanks to <a href="https://leafletjs.com/">Leaflet</a>. To create your own visualization, go to <a href="https://geojson.io/">geojson.io</a>.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">geojson
</span><span class="sb">{
  "type": "FeatureCollection",
  "features": [
    {
      "type": "Feature",
      "properties": {},
      "geometry": {
        "coordinates": [
          [
            [
              -60.11363029935569,
              -2.904625022183211
            ],
            [
              -60.11363029935569,
              -3.162613728707967
            ],
            [
              -59.820894493858034,
              -3.162613728707967
            ],
            [
              -59.820894493858034,
              -2.904625022183211
            ],
            [
              -60.11363029935569,
              -2.904625022183211
            ]
          ]
        ],
        "type": "Polygon"
      }
    }
  ]
}</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-geojson">{
  "type": "FeatureCollection",
  "features": [
    {
      "type": "Feature",
      "properties": {},
      "geometry": {
        "coordinates": [
          [
            [
              -60.11363029935569,
              -2.904625022183211
            ],
            [
              -60.11363029935569,
              -3.162613728707967
            ],
            [
              -59.820894493858034,
              -3.162613728707967
            ],
            [
              -59.820894493858034,
              -2.904625022183211
            ],
            [
              -60.11363029935569,
              -2.904625022183211
            ]
          ]
        ],
        "type": "Polygon"
      }
    }
  ]
}
</code></pre>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="charts"/><category term="maps"/><summary type="html"><![CDATA[this is what included geojson code could look like]]></summary></entry></feed>